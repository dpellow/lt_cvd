{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3318a9-2634-4381-8102-37a76c4d9994",
   "metadata": {},
   "source": [
    "# Process the data that is pulled by the OMOP query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e2dd4a-6ae5-4cd1-bd0a-5b3b71dc03a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/t125594uhn/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/cluster/home/t125594uhn/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/tmp/ipykernel_1147425/956546953.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f41ed60-b206-45d8-97c3-bae4af2c00d4",
   "metadata": {},
   "source": [
    "Set the below paths to the locations of the OMOP tables\n",
    "\n",
    "Set outpath to the path that output files should be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf61cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2f463d-e9d1-4477-8aab-b3955d1d5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the paths to the tables \n",
    "pats_path = 'test/pats.csv'\n",
    "smoke_path = 'test/smok.csv'\n",
    "inds_path = 'test/inds.csv'\n",
    "dhd_path = 'test/dhd.csv'\n",
    "events_path = 'test/events.csv'\n",
    "labs_path = 'test/labs.csv'\n",
    "meds_path = 'test/meds.csv'\n",
    "deaths_path = 'test/deaths.csv'\n",
    "\n",
    "outpath = 'test'\n",
    "\n",
    "# load the tables\n",
    "pats = pd.read_csv(pats_path)\n",
    "smoke = pd.read_csv(smoke_path)\n",
    "inds = pd.read_csv(inds_path)\n",
    "dhd = pd.read_csv(dhd_path)\n",
    "events = pd.read_csv(events_path)\n",
    "labs = pd.read_csv(labs_path)\n",
    "meds = pd.read_csv(meds_path)\n",
    "deaths = pd.read_csv(deaths_path)\n",
    "\n",
    "\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2565b8-7c50-4bc5-be53-87aec41bf569",
   "metadata": {},
   "source": [
    "Set below global variables as needed.\n",
    "e.g. set STUDY_CUTOFF_DATE as the data pull date\n",
    "e.g. set keywords, concept codes, icd codes etc to match what they are in your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81618466-7063-471a-bc76-da4606becf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables defining the codes that are needed to match different conditions, labs, etc (project_lists.py in the project code)\n",
    "\n",
    "STUDY_CUTOFF_DATE = '2025-08-25'\n",
    "CV_EVENT_GAP_DAYS = 180\n",
    "\n",
    "\n",
    "TRANS_KEYWORDS = ['orthotropic liver transplant', 'heterotropic liver transplant', 'liver transplant']\n",
    "TRANS_CONCEPT_LIST = [467458,2100972, 4067459]\n",
    "TRANS_DICT = {'liver transplant': [467458,2100972,4067459]} # put any codes mapping to a liver transplant here\n",
    "INV_TRANS = {y:x for x in TRANS_DICT.keys() for y in TRANS_DICT[x]}\n",
    "\n",
    "METAB_CODES = ['K75.81','K76.0']\n",
    "METAB_CONCEPT_CODES = [45533616,35208359]\n",
    "METAB_DICT = {'K75.81' : [45533616],\n",
    "              'K76.0' : [35208359]  \n",
    "}\n",
    "INV_METAB = {y:x for x in METAB_DICT.keys() for y in METAB_DICT[x]}\n",
    "\n",
    "\n",
    "ALD_CODES = ['K70']\n",
    "ALD_CONCEPT_CODES = [35208330,35208331,35208332,45562507,45552946,45605952,\\\n",
    "                    45538545,45586730,45552947]\n",
    "ALD_DICT = {'K70' : [35208330,35208331,35208332,45562507,45552946,45605952,\\\n",
    "                    45538545,45586730,45552947]}\n",
    "INV_ALD = {y:x for x in ALD_DICT.keys() for y in ALD_DICT[x]}\n",
    "\n",
    "\n",
    "CANCER_CODES = ['C22']\n",
    "CANCER_CONCEPT_CODES = [35206146,35206147,35206148,35206149,35206150,35206151,\\\n",
    "                        45585987,35206152]\n",
    "CANCER_DICT = {'C22' : [35206146,35206147,35206148,35206149,35206150,35206151,\\\n",
    "                        35206152,45585987]}\n",
    "INV_CANCER = {y:x for x in CANCER_DICT.keys() for y in CANCER_DICT[x]}\n",
    "\n",
    "\n",
    "HEP_CODES = [\"B15\", \"B16\", \"B17\", \"B18\", \"B19\"]\n",
    "HEP_CONCEPT_CODES = list(range(35205759,35205776))+\\\n",
    "                        [45576259,45605221,45552212,45566540,45581151]\n",
    "HEP_DICT = {'B15' : [35205759,35205760],\n",
    "            'B16' : [35205761,35205762,35205763,35205764],\n",
    "            'B17' : [35205765,35205766,35205767,35205768,45576259,45605221],\n",
    "            'B18' : [35205769,35205770,35205771,35205772,35205773],\n",
    "            'B19' : [35205775,45552212,45566540,45581151,45547442 ]\n",
    "}\n",
    "INV_HEP = {y:x for x in HEP_DICT.keys() for y in HEP_DICT[x]}\n",
    "              \n",
    "                        \n",
    "FULM_CODES = [\"K72.0\"]\n",
    "FULM_CONCEPT_CODES = [45567324,45543338]\n",
    "FULM_DICT = {'K72.0' : [45567324,45543338]}\n",
    "INV_FULM = {y:x for x in FULM_DICT.keys() for y in FULM_DICT[x]}\n",
    "\n",
    "\n",
    "IMMUNE_CODES = [\"K75.4\", \"K74.3\", \"K83.01\"]\n",
    "IMMUNE_CONCEPT_CODES = [35208356,35208349] # <-- TODO: add PSC\n",
    "IMMUNE_DICT = {'K75.4' : [35208356],\n",
    "               'K74.3' : [35208349],\n",
    "               'K83.01' : []} # <-- TODO: add concept code PSC\n",
    "INV_IMMUNE = {y:x for x in IMMUNE_DICT.keys() for y in IMMUNE_DICT[x]}\n",
    "\n",
    "\n",
    "RE_TX_CODES = [\"Z94.4\", \"T86.4\"] # <-- TODO: consider taking out Z94.4, \"History of liver transplant\"\n",
    "RE_TX_CONCEPT_CODES = [45561100,45551553,45594968,45609395,45590132,35225408]\n",
    "RE_TX_DICT = {'Z94.4' : [35225408],\n",
    "              'T86.4' : [45561100,45551553,45594968,45609395,45590132]}\n",
    "INV_RE_TX = {y:x for x in RE_TX_DICT.keys() for y in RE_TX_DICT[x]}\n",
    "\n",
    "\n",
    "NONSMOKER_CODE = 40770157\n",
    "SMOKER_CODES = [40764333,40768543] # <-- current or former smoker\n",
    "SMOKER_CONCEPT_CODES = [42709996,762499,4310250,4298794,37395605,762498]\n",
    "SMOKER_DICT = {\"SMOKER\" : [42709996,762499,4310250,4298794,37395605,762498]}\n",
    "SMOKER_INV = {y:x for x in SMOKER_DICT.keys() for y in SMOKER_DICT[x]}\n",
    "\n",
    "DM_CODES = ['E10', 'E11']\n",
    "DM_CONCEPT_CODES = [1326492,1326493,35206878,35206879,35206881,35206882,37200148,37200166,37200167,37200170] + \\\n",
    "                    list(range(37200191,37200255))+list(range(45533017,45533024))+[45537962,45542736,45542737,45542738] + \\\n",
    "                    list(range(45547621,45547628)) + [45552379,45552382,45552385,45557113]+[45561949,45566731,45576439,45576443,\\\n",
    "                    45581350] + list(range(45581352,45581356)) + [45586139, 45586140, 45591027, 45591029, 45591031] + \\\n",
    "                    list(range(45595795,45595800)) + list(range(45600636,45600643)) + list(range(45605398,45605405))\n",
    "DM_DICT = {'E10' : [35206878,35206879,37200148,37200166,37200167,37200170,45533017,455330178,45542736,45542737,\\\n",
    "                    45547621,45547622,45547624,45552379,45552382,45576439,45581350,45595795,45600636,\\\n",
    "                    45600637,45600638,45600640,45605398],\n",
    "           'E11' : [1326492,1326493,35206881,35206882,]+[x for x in range(37200191,37200255)]+\n",
    "                   [45533019,45533020,45533021,45533022,45533023,45537962,45542738,\\\n",
    "                    45547625,45547626,45547627,45552385,45557113,45561949,45566731,45576443,\\\n",
    "                    45581352,45581353,45581354,45581355,45586139,45586140,45591027, 5591029,45591031,\\\n",
    "                    45595797,45595798,45595799,45600641,45600642,45605401,45605402,45605403,45605404]}\n",
    "INV_DM = {y:x for x in DM_DICT.keys() for y in DM_DICT[x]}\n",
    "    \n",
    "\n",
    "HTN_CODES = ['I10', 'I11', 'I12', 'I13', 'I15']\n",
    "HTN_CONCEPT_CODES = list(range(35207668,35207679))\n",
    "HTN_DICT = {'I10' : [35207668],\n",
    "            'I11' : [35207669,35207670],\n",
    "            'I12' : [35207671,35207672],\n",
    "            'I13' : [35207673,35207674],\n",
    "            'I15' : [35207675,35207676,35207677,35207678,35207679]}\n",
    "INV_HTN = {y:x for x in HTN_DICT.keys() for y in HTN_DICT[x]}\n",
    "\n",
    "\n",
    "LIP_CODES = ['E78.0', 'E78.1', 'E78.2', 'E78.5']\n",
    "LIP_CONCEPT_CODES = list(range(35207060,35207063))+[35207065,37200312,37200313]\n",
    "LIP_DICT = {'E78.0' : [35207060,37200312,37200313],\n",
    "            'E78.1' : [35207061],\n",
    "            'E78.2' : [35207062],\n",
    "            'E78.5' : [35207065]}\n",
    "INV_LIP = {y:x for x in LIP_DICT.keys() for y in LIP_DICT[x]}\n",
    "\n",
    "\n",
    "CV_CODES = ['I48.0', 'I48.1', 'I48.2', 'I48.3', 'I48.4', 'I48.9', 'I21', 'I22', 'I25.2',\\\n",
    "            'I50', 'I46', 'G45.3','G45.9', 'I25.1','I25.4', 'I25.5', 'I25.6','I25.7','I25.8','I25.9', \\\n",
    "            'I63', 'I34', 'I35', 'I36', 'I37', 'I69.3', 'I42', 'I65', 'I66', 'I67.0'] # 'I39.0', 'I39.1', 'I39.2', 'I39.3', 'I39.4', # 'I47', 'I49',\n",
    "CV_CONCEPT_CODES = [35207396,35207399,45562340,35207396,35207399,45562340,45572079,35207684,35207685,45576865,1326588,45533436,\\\n",
    "                    45605779,45572080,45557536,1326590,1326591,35207686,45605781,35207702,35207703,35207704,35207705,35207706,\\\n",
    "                    45586572,45557538,45538373,45596199,45596199,45548013,45567168,785999,45605788,45601024,45591456,37402491,\\\n",
    "                    45576866,45596197,45548010,45601027,45605784,45543167,45586574,45548012,45567167,45562344,45605787,35207755,\\\n",
    "                    35207756,35207757,35207758,35207759,35207760,35207761,35207762,35207763,35207764,45572091,45538383,35207779,\\\n",
    "                    35207781,35207782,35207783,786000,786001,786002,37402490,37402503,37402504,35207784,35207785,1569171,1569172,\\\n",
    "                    1569173,1553751,1553752,1553753,1553754,45576876,45572094,45562353,35207786,35207787,35207788,35207789,\\\n",
    "                    35207790,35207791,45591468,35207792,35207793,45586587,45543182,45576878,45567180,45601038,45548022,45533456,\\\n",
    "                    45562355,45533457,45591469,45586588,45567181,1326606,1326607,1326608,1326609,1326601,1326602,1326603,1326604,\\\n",
    "                    1326605,45601041,35207820,45533463,35207821,45567187,45581782,37200496,45562362,45605806,45552802,45538396,\\\n",
    "                    45591474,45562363,45576888,45552803,45601045,45552806,45601047,1595597,1595598,37200498,45596212,37200499,\\\n",
    "                    45576885,45601043,45562365,45543187,45567188,37200502,45557553,45576887,45596213,45586593,45562367,45586594,\\\n",
    "                    37200506,45576889,45543190,37200507,45596214,45548029,45548030,37200508,45581783,45601044,45552805,37200509,\\\n",
    "                    45605809,45586595,37200510,45538397,45586596,45557555,45586597,45581784,45596215,45586598,35207823,45552807,\\\n",
    "                    45601048,45557556,45601049,45601050,45557557,35207827,35207828,45601060,37200539,37200544,37200545,45548047,\\\n",
    "                    45538412,45557562,45533478,45538413,45548048,45586611,45591489,45543202,45533479,45572111,45557563,45557564,\\\n",
    "                    45548049,45605822,45596223]\n",
    "CV_DICT = {'I48.0':[35207784],\n",
    "           'I48.1':[35207785,1553751,1553752],\n",
    "           'I48.2':[1569171,1553753,1553754],\n",
    "           'I48.3':[1569172],\n",
    "           'I48.4':[1569173],\n",
    "           'I48.9':[45576876,45572094],\n",
    "           'I47':[35207781,35207782,35207783,786000,786001,786002,37402490,37402503,37402504],\n",
    "           'I49':[45562353,35207786,35207787,35207788,35207789,35207790,35207791,45591468],\n",
    "           'I21':[45562340,45562340,45572079,35207684,35207685,45576865,1326588,45533436,45605779,\\\n",
    "                  45572080,45557536,1326590,1326591],\n",
    "           'I22':[35207686,45605781],\n",
    "           'I25.1':[45586572,45601024,45591456,37402491,45576866,45596197],\n",
    "           'I25.2':[35207702],\n",
    "           'I25.4':[45557538,45538373,],\n",
    "           'I25.5':[35207704],\n",
    "           'I25.6':[35207705],\n",
    "           'I25.7':[45548010,45601027,45605784,45543167,45586574,45548012],\n",
    "           'I25.8':[45596199,45548013,45567168,785999,45605788,45567167,45562344,45605787],\n",
    "           'I25.9':[35207706],\n",
    "           'I50':[35207792,35207793,45586587,45543182,45576878,45567180,45601038,45548022,45533456,\\\n",
    "                  45562355,45533457,45591469,45586588,45567181,1326606,1326607,1326608,1326609,1326601,\\\n",
    "                  1326602,1326603,1326604,1326605],\n",
    "           'I46':[45572091,45538383,35207779],\n",
    "           'G45.3':[35207396],\n",
    "           'G45.9':[35207399],\n",
    "           'I63':[45601041,35207820,45533463,35207821,45567187,45581782,37200496,45562362,45605806,45552802,45538396,\\\n",
    "                  45591474,45562363,45576888,45552803,45601045,45552806,45601047,1595597,1595598,37200498,45596212,37200499,\\\n",
    "                  45576885,45601043,45562365,45543187,45567188,37200502,45557553,45576887,45596213,45586593,45562367,45586594,\\\n",
    "                  37200506,45576889,45543190,37200507,45596214,45548029,45548030,37200508,45581783,45601044,45552805,37200509,\\\n",
    "                  45605809,45586595,37200510,45538397,45586596,45557555,45586597,45581784,45596215],\n",
    "           'I65':[45586598,35207823,45552807,5601049,45601050],\n",
    "           'I66':[45557557,35207827],\n",
    "           'I67.0':[35207828],\n",
    "           'I69.3':[45601060,37200539,37200544,37200545,45548047,45538412,45557562,45533478,45538413,45548048,45586611,45591489,\\\n",
    "                    45543202,45533479,45572111,45557563,45557564,45548049,45605822,45596223],\n",
    "       #     'I39.0':[], # <-- I think all of these have been reclassified\n",
    "       #     'I39.1':[],\n",
    "       #     'I39.2':[],\n",
    "       #     'I39.3':[], \n",
    "       #     'I39.4':[],\n",
    "           'I34':[], # <-- TODO: add these\n",
    "           'I35':[],\n",
    "           'I36':[],\n",
    "           'I37':[],\n",
    "           'I42':[35207755,35207756,35207757,35207758,35207759,35207760,35207761,35207762,35207763,35207764]}\n",
    "CV_INV = {y:x for x in CV_DICT.keys() for y in CV_DICT[x]}\n",
    "\n",
    "\n",
    "CAD_CHRONIC_CODES = ['I25.1','I25.4','I25.6','I25.7','I25.8','I25.9']\n",
    "ARYTHMIA_CHRONIC_CODES = ['I48.0', 'I48.1', 'I48.2','I48.91']\n",
    "HEART_FAIL_CHRONIC_CODES = ['I25.5','I50','I42']\n",
    "VALV_CHRONIC_CODES = ['I34','I35','I36','I37']\n",
    "CEREBRO_CHRONIC_CODES = ['I69.3']\n",
    "\n",
    "\n",
    "ARYTHMIA_CODES = ['I48.0', 'I48.1', 'I48.2', 'I48.3', 'I48.4', 'I48.9']#, 'I47', 'I49']\n",
    "ACS_CODES = ['I21','I22', 'I25.2', 'I46']\n",
    "CAD_CODES = ['I25.1','I25.4','I25.6','I25.7','I25.8','I25.9']\n",
    "VALV_CODES = ['I34','I35','I36','I37']\n",
    "CEREBRO_CODES = ['G45.3', 'G45.9','I63','I69.3', 'I65', 'I66', 'I67.0']\n",
    "HF_CODES = ['I25.5','I50','I42']\n",
    "\n",
    "LABS_DICT = {\n",
    "    'CREATININE': [3016723],\n",
    "    'ALP': [3035995],\n",
    "    'ALT': [3006923],\n",
    "    'CYCLO': [3010375],\n",
    "    'AST': [3013721],\n",
    "    'TAC': [3026250],\n",
    "    'BMI': [3038553]\n",
    "}\n",
    "LABS_INV = {y:x for x in LABS_DICT.keys() for y in LABS_DICT[x]}\n",
    "\n",
    "\n",
    "CREATININE_ID = 3016723\n",
    "ALP_ID = 3035995\n",
    "ALT_ID = 3006923\n",
    "CYCLO_ID = 3010375\n",
    "AST_ID = 3013721\n",
    "TAC_ID = 3026250\n",
    "BMI_ID = 3038553\n",
    "\n",
    "LIP_LAB_IDS = {\n",
    "    'LDL': [3028288,3028437],\n",
    "    'HDL':[], # TODO: Do we have this?\n",
    "    'TOTAL_CHOLESTEROL': [3027114],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "MEDS_DICT = {'ANTI_HTN': [779445,964322,964324,964325,974858,978556,978557,1308221,1308250,1308251,1314006,1314008,1314009,\\\n",
    "                        1314581,1314614,1317675,1328689,1331312,1332419,1332497,1332500,1332525,1332526,1332527,1332528,\\\n",
    "                        1334459,1334460,1334494,1334535,1340161,1341268,1341270,1341302,1350490,1350521,1350552,1351558,\\\n",
    "                        1351559,1351583,1351587,1361519,1363057,1363058,1363059,1363060,1395060,19018811,19019236,19019238,\\\n",
    "                        19019239,19022947,19022948,19022949,19028935,19028936,19029027,19067686,19073093,19073094,19074671,\\\n",
    "                        19074672,19074673,19078080,19078101,19080128,19080129,19101573,19124265,19127430,19127432,19127433,\\\n",
    "                        19133212,19133558,19133562,19133566,19133570,19133574,19133578,19133582,19133585,19133587,19133614,\\\n",
    "                        19133621,19133622,35605001,35605003,40162864,40162867,40162871,40162875,40162878,40165757,40165762,\\\n",
    "                        40165767,40165773,40165785,40165789,40166824,40166826,40166828,40166830,40167087,40167091,40167196,\\\n",
    "                        40167202,40167213,40167218,40167849,40171499,40171510,40171516,40171547,40171550,40171553,40171556,\\\n",
    "                        40171559,40171562,40171849,40171852,40171863,40171884,40171905,40171917,40184184,40184187,40184217,\\\n",
    "                        40185276,40185280,40185304,40221243,40224172,40224175,40224178,42629595,42629596,42629597,42629598,\\\n",
    "                        43560163,46221722,46221724,46287342,46287346],\n",
    "             'ANTI_PLATELET': [1112841,1112892,1112922,1113346,1322189,1331312,19046742,\\\n",
    "                               19059056,19065472,19066057,19073712,19075601,19076600,19076621,19076623,35605960,40163720,\\\n",
    "                               40163724,40241188,46287538],\n",
    "             'STATIN': [1526476,1526480,1539407,1539411,1539462,1539463,1545959,1545996,1545997,1551927,1551929,1552015,\\\n",
    "                        19019115,19019116,19019117,19023487,19077244,19077245,19077497,19077498,19098474,19112569,19123592,\\\n",
    "                        19129329,40165245,40165246,40165253,40165257,40165261,40165262,40165638,40165642,40175390,40175394,\\\n",
    "                        40175400,40175404]\n",
    "          }\n",
    "MEDS_INV = {y:x for x in MEDS_DICT.keys() for y in MEDS_DICT[x]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce3d41d-4f47-4719-adaa-145124f1db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the cohort demographic info \n",
    "\n",
    "# 1. define the funcs\n",
    "\n",
    "def process_pats(pats):\n",
    "    ''' PATS table processing.\n",
    "        columns:\n",
    "            - person_id\n",
    "            - birth_date -> convert to datetime\n",
    "            - transplant_date -> convert to datetime. Take the last transplant per patient\n",
    "            - sex -> convert to binary: M=0, F=1. Remove any other values\n",
    "            - procedure_name -> check that it's in the list and drop columns that aren't      \n",
    "        new colums:\n",
    "            - age_at_trans -> calculate from birth_date and transplant_date\n",
    "    '''\n",
    "    pats['birth_date'] = pd.to_datetime(pats['birth_date'], format='mixed')\n",
    "    pats['transplant_date'] = pd.to_datetime(pats['transplant_date'], format='mixed')\n",
    "    \n",
    "    # keep only the last transplant per patient\n",
    "    pats = pats.sort_values('transplant_date').groupby('person_id').tail(1)\n",
    "    \n",
    "    pats['sex'] = pats['sex'].apply(lambda x: 0 if x == 'Male' else 1 if x == 'Female' else np.nan)\n",
    "        \n",
    "    # drop pats with sex=nan and print warning\n",
    "    missing_sex = pats.loc[pats['sex'].isna(), 'person_id'].values.tolist()\n",
    "    if(len(missing_sex) > 0):\n",
    "        print(f\"Dropping {len(missing_sex)} patients with missing sex values:\")\n",
    "        print(missing_sex)\n",
    "    pats = pats[pats['sex'].notna()]\n",
    "    \n",
    "    # drop pats with procedure_name not in list and print warning\n",
    "    nontx_pats = pats.loc[~(pats['procedure_name'].str.contains('liver transplant', case=False)), 'person_id'].values.tolist()\n",
    "    if len(nontx_pats) > 0:\n",
    "        print(f\"Dropping {len(nontx_pats)} patients with non-transplant procedure names:\")\n",
    "        print(nontx_pats)\n",
    "    pats = pats[pats['procedure_name'].str.contains('liver transplant', case=False)]\n",
    "    pats = pats.drop(columns=['procedure_name'])\n",
    "    \n",
    "    pats['age_at_tx'] = ((pats['transplant_date'] - pats['birth_date']).dt.days / 365.25).round(2)\n",
    "    pats = pats.drop(columns=['birth_date'])\n",
    "        \n",
    "    # drop pats < 18 at tx\n",
    "    if len(pats[pats['age_at_tx'] < 18]) > 0:\n",
    "        print(f\"Dropping {len(pats[pats['age_at_tx'] < 18])} patients < 18 at transplant:\")\n",
    "    pats = pats[pats['age_at_tx'] >= 18]\n",
    "    \n",
    "    return pats\n",
    "\n",
    "\n",
    "def process_deaths(cohort, deaths):\n",
    "    ''' DEATHS_CTE table processing.\n",
    "        columns:\n",
    "            - person_id\n",
    "            - death_date -> convert to datetime\n",
    "        new colums:\n",
    "            - CENSOR_DATE -> date of death or study end date for those with no death date\n",
    "            \n",
    "    '''\n",
    "    deaths['death_date'] = pd.to_datetime(deaths['death_date'], format='mixed')\n",
    "    # merge with cohort on person_id\n",
    "    cohort = pd.merge(cohort, deaths[['person_id', 'death_date']], on='person_id', how='left')\n",
    "    # set the censor date to the death date or study end date\n",
    "    cohort['CENSOR_DATE'] = cohort['death_date'].fillna(pd.to_datetime(STUDY_CUTOFF_DATE))\n",
    "    # drop the death date column\n",
    "    cohort = cohort.drop(columns=['death_date'])\n",
    "    \n",
    "    # drop patients censored less than 1.25 years post-transplant\n",
    "    censored = cohort[cohort['CENSOR_DATE'] < (cohort['transplant_date'] + pd.DateOffset(years=1, months=3))]\n",
    "    if len(censored) > 0:\n",
    "        print(f\"Dropping {len(censored)} patients with censor date < 1.25 years post-transplant:\")\n",
    "        print(censored['person_id'].values.tolist())\n",
    "\n",
    "    cohort = cohort[cohort['CENSOR_DATE'] >= (cohort['transplant_date'] + pd.DateOffset(years=1, months=3))]\n",
    "\n",
    "    return cohort\n",
    "\n",
    "\n",
    "\n",
    "# 2. run the funcs\n",
    "\n",
    "cohort = process_pats(pats)\n",
    "cohort = process_deaths(cohort, deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fdd425-9cba-4e2c-b20d-46f3c50c524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process smoking info\n",
    "\n",
    "\n",
    "# funcs\n",
    "def process_smoke(cohort, smoke):\n",
    "    ''' SMOKING_CTE table processing.\n",
    "        columns:\n",
    "            - person_id\n",
    "            - smoking_status -> codes for current, former, never.\n",
    "            - observation_date\n",
    "        new columns:\n",
    "            - SMOKER -> binary: 0 if never, 1 else.\n",
    "    '''\n",
    "    smoker_ids = smoke.loc[smoke['smoking_status'].isin(SMOKER_CONCEPT_CODES), 'person_id'].values.tolist()\n",
    "    cohort['SMOKER'] = cohort['person_id'].apply(lambda x: 1 if x in smoker_ids else 0)\n",
    "    \n",
    "    return cohort\n",
    "\n",
    "# run\n",
    "\n",
    "cohort = process_smoke(cohort, smoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8294203c-05a6-484e-aeaf-dfa9921e6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process transplant indications\n",
    "\n",
    "# funcs\n",
    "def process_inds(cohort, inds, pats):\n",
    "    ''' INDICATION_CTE table processing.\n",
    "        columns:\n",
    "            - person_id\n",
    "            - diagnosis_date -> convert to date, keep only the one at the final transplant\n",
    "            - diagnosis\n",
    "            - icd10_code -> map from codes to indication columns\n",
    "            \n",
    "        new columns:\n",
    "            - METAB -> metabolic syndrome as indication\n",
    "            - ALD -> alcoholic liver disease as indication\n",
    "            - CANCER -> HCC as indication\n",
    "            - HEP -> hepatitis as indication\n",
    "            - FULM -> fulminant liver failure as indication\n",
    "            - IMMUNE -> autoimmune liver disease as indication\n",
    "            - RE_TX -> re-transplant as indication\n",
    "    '''\n",
    "    inds['diagnosis_date'] = pd.to_datetime(inds['diagnosis_date'], format='mixed')\n",
    "    \n",
    "    # the code can be a prefix of the full code, so we need to check for that\n",
    "    inds['METAB'] = inds['icd10_code'].apply(lambda x: 1 if any([x.startswith(c) for c in METAB_CODES]) else 0)    \n",
    "    inds['ALD'] = inds['icd10_code'].apply(lambda x: 1 if any([x.startswith(c) for c in ALD_CODES]) else 0)\n",
    "    inds['CANCER'] = inds['icd10_code'].apply(lambda x: 1 if any([x.startswith(c) for c in CANCER_CODES]) else 0)\n",
    "    inds['HEP'] = inds['icd10_code'].apply(lambda x: 1 if any([x.startswith(c) for c in HEP_CODES]) else 0)\n",
    "    inds['FULM'] = inds['icd10_code'].apply(lambda x: 1 if any([x.startswith(c) for c in FULM_CODES]) else 0)\n",
    "    inds['IMMUNE'] = inds['icd10_code'].apply(lambda x: 1 if any([x.startswith(c) for c in IMMUNE_CODES]) else 0)\n",
    "    ## do RE_TX separately using the pats table !\n",
    "    ## inds['RE_TX'] = inds['icd10_code'].apply(lambda x: 1 if any([x.startswith(c) for c in RE_TX_CODES]) else 0)\n",
    "    \n",
    "    # merge all the rows of each patient into a single row\n",
    "    inds = inds.groupby('person_id').agg({'METAB':'max', 'ALD':'max', 'CANCER':'max', 'HEP':'max', 'FULM':'max', 'IMMUNE':'max', 'diagnosis_date':'min'}).reset_index()    \n",
    "    \n",
    "    ## C-S cohort has unexpectedly very high number of FULM - this is unlikely,\n",
    "    ## probably these were coded with K72.0 due to some difference in coding practice\n",
    "    ## Exclude any of these that are also one of the other conditions\n",
    "    inds['FULM'] = (inds['FULM'] & ~(inds['METAB'] | inds['ALD'] | inds['CANCER'] | inds['HEP'] | inds['IMMUNE'])).astype(int)\n",
    "        \n",
    "    \n",
    "    cohort = pd.merge(cohort, inds, on='person_id', how='left')\n",
    "    # TODO: Figure out best way to do this date filtering.\n",
    "    # pre_tx_timedelta = (cohort['transplant_date'] - cohort['diagnosis_date']).dt.days\n",
    "    # # cohort = cohort.loc[((cohort['RE_TX']==1)&(pre_tx_timedelta>1))|\n",
    "    # #                     ((pre_tx_timedelta <= 365)&(pre_tx_timedelta >= 0))]\n",
    "    # cohort = cohort.loc[(pre_tx_timedelta >= 0)]\n",
    "        \n",
    "    cohort = cohort.drop(columns = ['diagnosis_date'])\n",
    "    \n",
    "    pats['birth_date'] = pd.to_datetime(pats['birth_date'], format='mixed')\n",
    "    pats['transplant_date'] = pd.to_datetime(pats['transplant_date'], format='mixed')\n",
    "    # keep only the last transplant per patient\n",
    "    pats = pats.sort_values(['person_id','transplant_date'])\n",
    "    def had_prior_tx(subdf):\n",
    "        last_date = subdf['transplant_date'].dt.normalize().iloc[-1]\n",
    "        earlier_dates = subdf['transplant_date'].dt.normalize() < last_date  # strictly earlier\n",
    "        return int(earlier_dates.any())\n",
    "\n",
    "    indicator = (\n",
    "        pats.groupby('person_id')\n",
    "        .apply(had_prior_tx)\n",
    "        .rename('RE_TX')\n",
    "    )\n",
    "    \n",
    "    cohort = cohort.merge(indicator, left_on='person_id', how='left', right_index=True)\n",
    "    \n",
    "    return cohort\n",
    "\n",
    "# run\n",
    "cohort = process_inds(cohort, inds, pats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927b08a2-c980-4233-a459-2c84c8e9a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process health statuses\n",
    "\n",
    "# funcs\n",
    "\n",
    "# Helper function to apply conditions across time\n",
    "def mark_condition(cohort, dhd, condition_name, code_list):\n",
    "    end_date = pd.to_datetime(STUDY_CUTOFF_DATE)\n",
    "    max_years = int((end_date - cohort['transplant_date'].min()).days / 365.25)\n",
    "    condition_df = dhd[dhd['icd10_code'].str.startswith(tuple(code_list))]\n",
    "    merged = condition_df.merge(cohort[['person_id', 'transplant_date']], on='person_id', how='left')\n",
    "    merged['years_since_tx'] = ((merged['diagnosis_date'] - merged['transplant_date']).dt.days / 365.25)\n",
    "\n",
    "    for i in range(1, max_years + 1):\n",
    "        hits = merged.loc[merged['years_since_tx'] < (i+0.25), 'person_id'].unique()\n",
    "        cohort[f'{condition_name}_{i}'] = cohort['person_id'].isin(hits).astype(int)\n",
    "    return cohort\n",
    "\n",
    "\n",
    "def process_dhd(cohort, dhd):\n",
    "    ''' DHD_CTE table processing.\n",
    "        columns:\n",
    "            - person_id\n",
    "            - diagnosis_date -> convert to date, keep only the one at the final transplant\n",
    "            - diagnosis\n",
    "            - icd10_code -> map from codes to indication columns\n",
    "            \n",
    "        new columns:\n",
    "            - DIABETES_<yr> -> diabetes as diagnosis\n",
    "            - HYPERTENSION_<yr> -> hypertension as diagnosis\n",
    "            - DYSLIPIDEMIA_<yr> -> dyslipidemia as diagnosis\n",
    "            \n",
    "            \n",
    "        NOTE: we construct the columns for each year from the transplant date.\n",
    "        NOTE: Years are offset by 3 months, basically we start counting from 3 months post-tx.         \n",
    "    '''\n",
    "    # for each patient create columns DM_1, DM_2,... HTN_1... LIP_1... \n",
    "    # up to now from 3 months after the transplant\n",
    "    dhd['diagnosis_date'] = pd.to_datetime(dhd['diagnosis_date'], format='mixed')\n",
    "\n",
    "    # Apply for each condition\n",
    "    cohort = mark_condition(cohort, dhd, 'DM', DM_CODES)\n",
    "    cohort = mark_condition(cohort, dhd, 'HTN', HTN_CODES)\n",
    "    cohort = mark_condition(cohort, dhd, 'LIP', LIP_CODES)\n",
    "\n",
    "    return cohort\n",
    "\n",
    "\n",
    "# run\n",
    "cohort = process_dhd(cohort, dhd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49ff5946-cc7a-4aca-8fd2-ef837922b474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1147425/3249410689.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cohort[colname] = np.nan\n",
      "/tmp/ipykernel_1147425/3249410689.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cohort[colname] = np.nan\n",
      "/tmp/ipykernel_1147425/3249410689.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cohort[colname] = np.nan\n",
      "/tmp/ipykernel_1147425/3249410689.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cohort[colname] = np.nan\n"
     ]
    }
   ],
   "source": [
    "# process CV events\n",
    "\n",
    "# funcs\n",
    "def match_chronic(df,codes):    \n",
    "    is_match = df['icd10_code'].str.startswith(tuple(codes))\n",
    "    # Get first match per patient\n",
    "    first_match = df[is_match].groupby('person_id', as_index=False).first()\n",
    "    # Get all non-matching rows\n",
    "    non_match = df[~is_match]\n",
    "    # Combine them\n",
    "    result = pd.concat([non_match, first_match], ignore_index=True).sort_values(['person_id', 'diagnosis_date'])\n",
    "    return result\n",
    "\n",
    "\n",
    "def group_events(df, gap=30):\n",
    "    df = df.sort_values(by = ['person_id','diagnosis_date']).reset_index(drop=True)\n",
    "    df_to_collapse = df[df[\"icd10_code\"].isin(CV_CODES)].copy()\n",
    "   # df_other = df[~df[\"icd10_code\"].isin(CV_CODES)].copy() # this should be empty\n",
    "\n",
    "    # compute gap\n",
    "    df_to_collapse[\"prev_date\"] = df_to_collapse.groupby([\"person_id\",\"icd10_code\"])[\"diagnosis_date\"].shift()\n",
    "    df_to_collapse[\"days_since_prev\"] = (df_to_collapse[\"diagnosis_date\"] - df_to_collapse[\"prev_date\"]).dt.days\n",
    "\n",
    "    # new cluster whenever first event or gap exceeded\n",
    "    df_to_collapse[\"new_cluster\"] = (df_to_collapse[\"days_since_prev\"].isna()) | (df_to_collapse[\"days_since_prev\"] > gap)\n",
    "\n",
    "    # cluster id\n",
    "    df_to_collapse[\"cluster_id\"] = df_to_collapse.groupby([\"person_id\",\"icd10_code\"])[\"new_cluster\"].cumsum()\n",
    "\n",
    "    # now: keep *first event in each cluster* only\n",
    "    collapsed = df_to_collapse.groupby([\"person_id\",\"icd10_code\",\"cluster_id\"]).first().reset_index(drop=True)\n",
    "\n",
    "    # combine back with unaffected events\n",
    "   # result = pd.concat([collapsed[[\"person_id\",\"icd10_code\",\"diagnosis_date\"]], df_other], ignore_index=True)\n",
    "    result = collapsed.sort_values([\"person_id\",\"diagnosis_date\"]).reset_index(drop=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def process_events(cohort, events):\n",
    "    ''' CV_EVENTS_CTE table processing.\n",
    "        columns:\n",
    "            - person_id\n",
    "            - diagnosis_date\n",
    "            - diagnosis \n",
    "            - icd10_code \n",
    "            \n",
    "        new columns:\n",
    "            - CV_HISTORY_<yr> -> past cardiovascular event as diagnosis\n",
    "            - MONTHS_TO_EVENT_<yr> -> months to event for each year\n",
    "    '''\n",
    "    events['diagnosis_date'] = pd.to_datetime(events['diagnosis_date'], format='mixed')\n",
    "    \n",
    "    # drop anything here that is not coded as a CV event\n",
    "    events = events[events['icd10_code'].str.startswith(tuple(CV_CODES))]\n",
    "    events = events[events['person_id'].isin(cohort['person_id'])]\n",
    "    \n",
    "    # try to group repeated event codes into a single event - window = 7 days\n",
    "    events = group_events(events, CV_EVENT_GAP_DAYS)\n",
    "    \n",
    "    cohort = mark_condition(cohort, events, 'CV_HISTORY', CV_CODES)\n",
    "    \n",
    "    # now we need to calculate the time to next event for each year\n",
    "    merged = events.merge(cohort[['person_id', 'transplant_date']], on='person_id', how='left')\n",
    "\n",
    "    # Get max duration\n",
    "    max_years = int((pd.to_datetime(STUDY_CUTOFF_DATE) - cohort['transplant_date'].min()).days / 365.25)\n",
    "\n",
    "    # Sort for quick lookup\n",
    "    merged = merged.sort_values(by=['person_id', 'diagnosis_date'])\n",
    "    \n",
    "    # chronic events are counted as events only the first time    \n",
    "    merged = match_chronic(merged,CAD_CHRONIC_CODES) \n",
    "    merged = match_chronic(merged,ARYTHMIA_CHRONIC_CODES)\n",
    "    merged = match_chronic(merged,VALV_CHRONIC_CODES)\n",
    "    merged = match_chronic(merged,HEART_FAIL_CHRONIC_CODES)\n",
    "    merged = match_chronic(merged,CEREBRO_CHRONIC_CODES)\n",
    "\n",
    "\n",
    "    # Loop through each follow-up year\n",
    "    for i in range(1, max_years + 1):\n",
    "        colname = f'MONTHS_TO_EVENT_{i}'\n",
    "        cohort[colname] = np.nan\n",
    "\n",
    "        for idx, row in cohort.iterrows():\n",
    "            pid = row['person_id']\n",
    "            anchor_date = row['transplant_date'] + pd.DateOffset(years=i,months=3)\n",
    "\n",
    "            # Get all  diagnoses for this patient after the anchor date\n",
    "            future_df = merged[(merged['person_id'] == pid) & (merged['diagnosis_date'] > anchor_date)]\n",
    "\n",
    "            if not future_df.empty:\n",
    "                next_event_date = future_df['diagnosis_date'].iloc[0]\n",
    "                time_to_event = (next_event_date - anchor_date).days / 30.4\n",
    "                cohort.at[idx, colname] = time_to_event\n",
    "        cohort[colname] = cohort[colname].round()\n",
    "                \n",
    "    return cohort, merged\n",
    "\n",
    "# run\n",
    "cohort, processed_events = process_events(cohort, events)\n",
    "\n",
    "\n",
    "def get_cv_event_debugging_info(cohort, events):\n",
    "    events['diagnosis_date'] = pd.to_datetime(events['diagnosis_date'], format='mixed')\n",
    "    \n",
    "    # drop anything here that is not coded as a CV event\n",
    "    events = events[events['icd10_code'].str.startswith(tuple(CV_CODES))]\n",
    "    events = events[events['person_id'].isin(cohort['person_id'])]\n",
    "    \n",
    "    # print off the following info:\n",
    "    # number of patients for which any given icd10_code occurs\n",
    "    # for the top ones of those: \n",
    "    #   # that were more that 1 year pre-transplant\n",
    "    #   # that were in the 1 year pre-transplat\n",
    "    #   # that were in the 1.25 years post-transplant\n",
    "    #   # that were more than 1.25 years post-transplant\n",
    "    \n",
    "    # get the unique icd10_codes per patient\n",
    "    code_stats = {}\n",
    "    for code in events['icd10_code'].unique():\n",
    "        code_df = events[events['icd10_code'] == code]\n",
    "        unique_pats = code_df['person_id'].nunique()\n",
    "        pre_1yr = 0\n",
    "        pre_tx = 0\n",
    "        post_1_25yr = 0\n",
    "        post_1_25yr_plus = 0\n",
    "        \n",
    "        for idx, row in code_df.iterrows():\n",
    "            pat_tx_date = cohort.loc[cohort['person_id'] == row['person_id'], 'transplant_date'].values[0]\n",
    "            delta_days = (row['diagnosis_date'] - pat_tx_date).days\n",
    "            \n",
    "            if delta_days < -365:\n",
    "                pre_1yr += 1\n",
    "            elif -365 <= delta_days < 0:\n",
    "                pre_tx += 1\n",
    "            elif 0 <= delta_days <= 456.5:\n",
    "                post_1_25yr += 1\n",
    "            else:\n",
    "                post_1_25yr_plus += 1\n",
    "        code_stats[code] = {'unique_patients': unique_pats,\n",
    "                            'pre_1yr': pre_1yr,\n",
    "                            'pre_tx': pre_tx,\n",
    "                            'post_1_25yr': post_1_25yr,\n",
    "                            'post_1_25yr_plus': post_1_25yr_plus}\n",
    "        pre_1yr_df = code_df[code_df['diagnosis_date'] < (pat_tx_date - pd.DateOffset(years=1))]\n",
    "        pre_tx_df = code_df[(code_df['diagnosis_date'] >= (pat_tx_date - pd.DateOffset(years=1))) & (code_df['diagnosis_date'] < pat_tx_date)]\n",
    "        post_1_25yr_df = code_df[(code_df['diagnosis_date'] >= pat_tx_date) & (code_df['diagnosis_date'] <= (pat_tx_date + pd.DateOffset(years=1, months=3)))]\n",
    "        post_1_25yr_plus_df = code_df[code_df['diagnosis_date'] > (pat_tx_date + pd.DateOffset(years=1, months=3))]\n",
    "        code_stats[code]['pre_1yr_unique'] = pre_1yr_df['person_id'].nunique()\n",
    "        code_stats[code]['pre_tx_unique'] = pre_tx_df['person_id'].nunique()\n",
    "        code_stats[code]['post_1_25yr_unique'] = post_1_25yr_df['person_id'].nunique()\n",
    "        code_stats[code]['post_1_25yr_plus_unique'] = post_1_25yr_plus_df['person_id'].nunique()\n",
    "        \n",
    "        \n",
    "    # sort by unique patients\n",
    "    sorted_stats = dict(sorted(code_stats.items(), key=lambda item: item[1]['unique_patients'], reverse=True))\n",
    "    print(\"CV Event ICD10 Code Stats:\")\n",
    "    for code, stats in sorted_stats.items():\n",
    "        print(f\"Code: {code}, Unique Patients: {stats['unique_patients']}, Pre-1yr: {stats['pre_1yr']}, Pre-1yr unique patients: {stats['pre_1yr_unique']}, \\\n",
    "                Pre-Tx: {stats['pre_tx']}, Pre-Tx unique patients: {stats['pre_tx_unique']} Post-1.25yr: {stats['post_1_25yr']}, Post-1.25yr unique patients: {stats['post_1_25yr_unique']}, \\\n",
    "                Post-1.25yr+: {stats['post_1_25yr_plus']}, Post-1.25yr+ unique patients: {stats['post_1_25yr_plus_unique']}\")\n",
    "sorted_stats_df = get_cv_event_debugging_info(cohort, events)\n",
    "sorted_stats_df.to_csv(os.path.join(outpath, 'cv_event_debugging_stats.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51f844-9be0-40a7-9912-621d4cbfb88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process labs\n",
    "\n",
    "# funcs\n",
    "def process_labs(cohort, labs):\n",
    "    ''' LABS_CTE table processing.\n",
    "        columns:\n",
    "            - person_id\n",
    "            - measurement_date\n",
    "            - test_name \n",
    "            - test_code\n",
    "            - test_value\n",
    "            - test_unit\n",
    "            \n",
    "        new columns:\n",
    "            - <lab>_<yr> -> lab value for each lab for each year\n",
    "        NOTE: forward fill labs\n",
    "        NOTE: labs we use: ALT, ALP, AST, BMI, CREATININE, CYCLO, TAC\n",
    "    '''\n",
    "    # filter out any labs from before 3 months post-tx\n",
    "    labs['measurement_date'] = pd.to_datetime(labs['measurement_date'], format='mixed')\n",
    "    lab_df = labs.merge(cohort[['person_id', 'transplant_date']], on='person_id', how='left')\n",
    "    labs = lab_df[lab_df['measurement_date'] >= (lab_df['transplant_date'] + pd.DateOffset(months=3))].copy()\n",
    "    labs = labs.sort_values(['person_id', 'measurement_date'])\n",
    "    lab_cols = ['ALT', 'ALP', 'AST', 'BMI', 'CREATININE', 'CYCLO', 'TAC']\n",
    "    \n",
    "    max_years = int((pd.to_datetime(STUDY_CUTOFF_DATE) - cohort['transplant_date'].min()).days / 365.25)\n",
    "    new_cols_df = pd.DataFrame(np.nan,index=cohort.index, columns=[f'{lab}_{i}' for lab in lab_cols for i in range(1, max_years + 1)])\n",
    "    cohort = pd.concat([cohort, new_cols_df], axis=1)\n",
    "    for lab in lab_cols:\n",
    "        lab_subset = labs[labs['test_code'].isin(LABS_DICT[lab])].copy()\n",
    "        for i in range(1, max_years + 1):\n",
    "            col_name = f'{lab}_{i}'\n",
    "\n",
    "            # Define the window of interest per patient\n",
    "            for idx, row in cohort.iterrows():\n",
    "                pid = row['person_id']\n",
    "                anchor_date = row['transplant_date'] + pd.DateOffset(years=i, months=3)\n",
    "\n",
    "                # Filter lab values before (or at) anchor date\n",
    "                labs_for_patient = lab_subset[lab_subset['person_id'] == pid]\n",
    "                labs_before = labs_for_patient[labs_for_patient['measurement_date'] <= anchor_date]\n",
    "\n",
    "                if not labs_before.empty:\n",
    "                    most_recent = labs_before.sort_values('measurement_date', ascending=False).iloc[0]['test_value']\n",
    "                    cohort.at[idx, col_name] = most_recent\n",
    "    for i in range(1, max_years + 1):\n",
    "        cohort[f'CREATININE_{i}'] = cohort[f'CREATININE_{i}']*88.42            \n",
    "\n",
    "    # adjust tac and cyclo:\n",
    "    # if *any* tac values for a patient across all years, set all their cyclo to 0.\n",
    "    # if no tac values for a patient, and any cyclo values, set all tac to 0.\n",
    "    \n",
    "    tac_cols = [f'TAC_{i}' for i in range(1, max_years + 1)]\n",
    "    cyclo_cols = [f'CYCLO_{i}' for i in range(1, max_years + 1)]\n",
    "    for idx, row in cohort.iterrows():\n",
    "        pid = row['person_id']\n",
    "        tac_values = [row[col] for col in tac_cols]\n",
    "        cyclo_values = [row[col] for col in cyclo_cols]\n",
    "        \n",
    "        if any([(x > 0)for x in tac_values]):\n",
    "            # set all cyclo values to 0\n",
    "            for col in cyclo_cols:\n",
    "                cohort.at[idx, col] = 0\n",
    "        elif not any([(x > 0) for x in tac_values]) and any([(x > 0)for x in cyclo_values]):\n",
    "            # set all tac values to 0\n",
    "            for col in tac_cols:\n",
    "                cohort.at[idx, col] = 0\n",
    "                \n",
    "    # MODIFICATION FOR UCSF COHORT: patients missing all labs\n",
    "    \n",
    "    lab_check_cols = [f'{lab}_{i}' for lab in lab_cols for i in range(1, max_years + 1)]\n",
    "    \n",
    "    # nan or 0 in all lab check cols\n",
    "    missing_all = cohort[(cohort[lab_check_cols].isna() | (cohort[lab_check_cols]==0)).all(axis=1)]\n",
    "    print(f\"{len(missing_all)} patients missing all lab values after 3 months post-transplant.\")\n",
    "    # missing all after 1 year\n",
    "    missing_after_1yr = cohort[(cohort[[col for col in lab_check_cols if not col.endswith('_1')]].isna() | (cohort[[col for col in lab_check_cols if not col.endswith('_1')]]==0)).all(axis=1)]\n",
    "    print(f\"{len(missing_after_1yr)} patients missing all lab values after 1 year, 3 months post-transplant.\")\n",
    "    print(\"Dropping patients missing all lab values after 1 year:\")\n",
    "    print(missing_after_1yr['person_id'].values.tolist())\n",
    "    print(\"Range of transplant dates for these patients:\")\n",
    "    print(missing_after_1yr['transplant_date'].describe())\n",
    "    \n",
    "    cohort = cohort[~cohort['person_id'].isin(missing_after_1yr['person_id'])]\n",
    "    \n",
    "    return cohort\n",
    "\n",
    "# run\n",
    "cohort = process_labs(cohort, labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a310fbe-27b4-431a-a888-414e735f8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process medications\n",
    "\n",
    "# funcs\n",
    "def process_meds(cohort, meds):\n",
    "    ''' MEDS_CTE table processing.\n",
    "        columns:\n",
    "            - person_id\n",
    "            - start_date\n",
    "            - end_date\n",
    "            - medication_name\n",
    "            - medication_code\n",
    "            - dosage\n",
    "            \n",
    "        new columns:\n",
    "            - ANTI_HTN_<yr>\n",
    "            - ANTI_PLATELET_<yr>\n",
    "            - STATIN_<yr>\n",
    "        NOTE: forward fill meds\n",
    "    '''\n",
    "    meds['start_date'] = pd.to_datetime(meds['start_date'], format='mixed')\n",
    "    meds = meds.sort_values(['person_id', 'start_date'])\n",
    "    \n",
    "    med_cols = ['ANTI_HTN', 'ANTI_PLATELET', 'STATIN']\n",
    "    \n",
    "    max_years = int((pd.to_datetime(STUDY_CUTOFF_DATE) - cohort['transplant_date'].min()).days / 365.25)\n",
    "\n",
    "    for med in med_cols:\n",
    "        med_subset = meds[meds['medication_code'].isin(MEDS_DICT[med])].copy()\n",
    "        # keep only the first medication per patient\n",
    "        med_subset = med_subset.groupby('person_id').first().reset_index()\n",
    "        cohort = cohort.merge(med_subset[['person_id', 'start_date']], on='person_id', how='left')\n",
    "        for i in range(1, max_years + 1):\n",
    "            col_name = f'{med}_{i}'\n",
    "            cohort[col_name] = 0\n",
    "            cohort[col_name] = (cohort['start_date'] <= (cohort['transplant_date'] + pd.DateOffset(years=i, months=3))).astype(int)\n",
    "            # Drop the start_date column\n",
    "        cohort = cohort.drop(columns=['start_date'])\n",
    "        \n",
    "    return cohort\n",
    "\n",
    "# run\n",
    "cohort = process_meds(cohort, meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5ac4bd1-ed30-41a0-901a-5da3f0956e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the health statuses using medication and lab info\n",
    "\n",
    "# funcs\n",
    "def add_dhd(cohort, labs):\n",
    "    ''' Additional updates to the diseases:\n",
    "        - Anyone on ANTI_HTN meds has HTN, anyone on STATIN has LIP\n",
    "        - Any LDL > 4.1 or tryglycerides > 2.3 or Total cholesterol > 5.2 inidicative of LIP \n",
    "    '''\n",
    "    \n",
    "    max_yrs = int((pd.to_datetime(STUDY_CUTOFF_DATE) - cohort['transplant_date'].min()).days / 365.25)\n",
    "    for i in range(1, max_yrs + 1):\n",
    "        cohort[f'HTN_{i}'] |= cohort[f'ANTI_HTN_{i}']\n",
    "        cohort[f'LIP_{i}'] |= cohort[f'STATIN_{i}']\n",
    "        \n",
    "    labs_subset = labs[labs['test_name'].isin([x for k in LIP_LAB_IDS.keys() for x in LIP_LAB_IDS[k]])].copy()\n",
    "    \n",
    "    # TODO: finish this with triglycerides and total cholesterol\n",
    "    labs_subset = labs_subset[(labs_subset['test_name'].isin(LIP_LAB_IDS['LDL'])&\\\n",
    "                                labs_subset['test_value'] > 4.1)]\n",
    "    labs_subset = labs_subset.sort_values(['person_id', 'measurement_date'])\n",
    "    # take the first irregular lab for each patient\n",
    "    labs_subset = labs_subset.groupby('person_id').first().reset_index()\n",
    "    # merge with cohort\n",
    "    cohort = cohort.merge(labs_subset[['person_id', 'measurement_date']], on='person_id', how='left')\n",
    "    # for each year, if the lab is before the anchor date, set LIP_<yr> to 1\n",
    "    for i in range(1, max_yrs + 1):\n",
    "        cohort[f'LIP_{i}'] |= (cohort['measurement_date'] <= (cohort['transplant_date'] + pd.DateOffset(years=i,months=3))).astype(int)\n",
    "    cohort.drop(columns=['measurement_date'], inplace=True)\n",
    "    \n",
    "    return cohort\n",
    "\n",
    "# run\n",
    "cohort = add_dhd(cohort, labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34bfd80f-dffa-4940-a20e-8ed582a595ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Females': 0, 'Males': 2, 'Age': {'Median': 40.44, 'Lower': 32.615, 'Upper': 48.265}, 'Current, ex-smokers': 1, 'Indications': {'METAB': 1, 'ALD': 1, 'CANCER': 1, 'HEP': 0, 'FULM': 0, 'IMMUNE': 0, 'RE_TX': 0}, 'DM': {'First': 0, 'Last': 0}, 'HTN': {'First': 0, 'Last': 1}, 'LIP': {'First': 0, 'Last': 0}, 'CV_HISTORY': {'First': 1, 'Last': 2}, 'ANTI_HTN': {'First': 0, 'Last': 1}, 'ANTI_PLATELET': {'First': 0, 'Last': 0}, 'STATIN': {'First': 0, 'Last': 0}, 'ALT': {'Missing, final': 0, 'Median, final': 62.5, 'Lower, final': 46.25, 'Upper, final': 78.75}, 'ALP': {'Missing, final': 0, 'Median, final': 80.0, 'Lower, final': 69.0, 'Upper, final': 91.0}, 'AST': {'Missing, final': 0, 'Median, final': 20.0, 'Lower, final': 19.0, 'Upper, final': 21.0}, 'BMI': {'Missing, final': 0, 'Median, final': 33.1, 'Lower, final': 32.55, 'Upper, final': 33.650000000000006}, 'CREATININE': {'Missing, final': 0, 'Median, final': 77.0, 'Lower, final': 73.0, 'Upper, final': 81.0}, 'CYCLO': {'Missing, final': 0.0, 'Median, final': nan, 'Lower, final': nan, 'Upper, final': nan}, 'TAC': {'Missing, final': 1.0, 'Median, final': 6.2, 'Lower, final': 6.2, 'Upper, final': 6.2}, 'CV_EVENTS': {'First': {'N': 2, 'Median': 78.5, 'Lower': 43.25, 'Upper': 113.75}, 'Total': {'N': 3, 'Arrhythmia': 0, 'Valvular': 0, 'ACS': 1, 'CAD': 0, 'Cerebrovascular': 1, 'Heart failure': 1}}}\n"
     ]
    }
   ],
   "source": [
    "# Save cohort information\n",
    "\n",
    "# funcs\n",
    "def get_cohort_info(cohort, processed_events, outdir):\n",
    "    ''' Dump demographic info and stats on the cohort to a json file.\n",
    "    '''\n",
    "    \n",
    "    demo_dict = {}\n",
    "    demo_dict['Females'] = cohort['sex'].sum()\n",
    "    demo_dict['Males'] = len(cohort) - demo_dict['Females']\n",
    "    demo_dict['Age'] = {'Median': cohort['age_at_tx'].median(), 'Lower': cohort['age_at_tx'].quantile(0.25),\n",
    "                        'Upper': cohort['age_at_tx'].quantile(0.75)}\n",
    "    demo_dict['Current, ex-smokers'] = cohort['SMOKER'].sum()\n",
    "    demo_dict['Indications'] = {x:cohort[x].sum() for x in ['METAB', 'ALD', 'CANCER', 'HEP', 'FULM', 'IMMUNE', 'RE_TX']}\n",
    "        \n",
    "    bin_varying = ['DM', 'HTN', 'LIP', 'CV_HISTORY', 'ANTI_HTN', 'ANTI_PLATELET', 'STATIN']\n",
    "    max_years = int((pd.to_datetime(STUDY_CUTOFF_DATE) - cohort['transplant_date'].min()).days / 365.25)\n",
    "    for v in bin_varying:\n",
    "        demo_dict[v] = {'First':cohort[f'{v}_1'].sum(), 'Last':cohort[f'{v}_{max_years}'].sum()}\n",
    "    \n",
    "    labs = ['ALT', 'ALP', 'AST', 'BMI', 'CREATININE', 'CYCLO', 'TAC']\n",
    "    for l in labs:\n",
    "        demo_dict[l] = {}\n",
    "        demo_dict[l]['Missing, final'] = cohort[f'{l}_{max_years}'].isna().sum()\n",
    "        demo_dict[l]['Median, final'] = cohort[f'{l}_{max_years}'].median()\n",
    "        demo_dict[l]['Lower, final'] = cohort[f'{l}_{max_years}'].quantile(0.25)        \n",
    "        demo_dict[l]['Upper, final'] = cohort[f'{l}_{max_years}'].quantile(0.75)\n",
    "    # for tac and cyclosporine median, upper, lower should be of the non-zero. Missing should include zeros:\n",
    "    for l in ['TAC', 'CYCLO']:\n",
    "        demo_dict[l]['Median, final'] = cohort[f'{l}_{max_years}'][cohort[f'{l}_{max_years}'] > 0].median()\n",
    "        demo_dict[l]['Lower, final'] = cohort[f'{l}_{max_years}'][cohort[f'{l}_{max_years}'] > 0].quantile(0.25)        \n",
    "        demo_dict[l]['Upper, final'] = cohort[f'{l}_{max_years}'][cohort[f'{l}_{max_years}'] > 0].quantile(0.75)\n",
    "        demo_dict[l]['Missing, final'] = cohort[f'{l}_{max_years}'].isna().sum() + cohort[f'{l}_{max_years}'][cohort[f'{l}_{max_years}'] == 0].sum()\n",
    "    \n",
    "    # number of first events for a patient, median, upper, and lower MONTHS_TO_EVENT - for first event\n",
    "    # number of rows with a non-null value in one of the MONTHS_TO_EVENT columns\n",
    "    # event_cols = [f'MONTHS_TO_EVENT_{i}' for i in range(1, max_years + 1)]\n",
    "    # events_df = cohort[event_cols]\n",
    "    \n",
    "    # first_event_times = events_df.apply(lambda row: row[row.notna()].iloc[0] if row.notna().any() else np.nan, axis=1)\n",
    "    # first_event_times = first_event_times.dropna()\n",
    "    first_event_times = cohort['MONTHS_TO_EVENT_1'].dropna()\n",
    "    median = first_event_times.median()\n",
    "    lower_q = first_event_times.quantile(0.25)\n",
    "    upper_q = first_event_times.quantile(0.75)\n",
    "    demo_dict['CV_EVENTS'] = {}\n",
    "    demo_dict['CV_EVENTS']['First'] = {'N':len(first_event_times),\n",
    "                                     'Median':median,\n",
    "                                     'Lower':lower_q, 'Upper':upper_q}\n",
    "    processed_events = processed_events[processed_events['diagnosis_date'] >= (processed_events['transplant_date'] + pd.DateOffset(months=15))]\n",
    "    processed_events = processed_events[processed_events['person_id'].isin(cohort['person_id'])]\n",
    "    # times between events for the same patient\n",
    "    times_df = processed_events[['person_id','diagnosis_date','transplant_date']].copy()\n",
    "    times_df = times_df.drop_duplicates(subset=['person_id','diagnosis_date'])\n",
    "    times_df = times_df.sort_values(by=['person_id','diagnosis_date']).reset_index(drop=True)\n",
    "    times_df[\"months_since_prev\"] = times_df.groupby(\"person_id\")[\"diagnosis_date\"].diff().dt.days / 30.4\n",
    "    times_df[\"months_since_prev\"] = times_df[\"months_since_prev\"].fillna((((times_df[\"diagnosis_date\"] - (times_df[\"transplant_date\"])).dt.days) / 30.4)-15)\n",
    "    median = times_df['months_since_prev'].median()\n",
    "    lower_q = times_df['months_since_prev'].quantile(0.25)\n",
    "    upper_q = times_df['months_since_prev'].quantile(0.75)\n",
    "    \n",
    "    demo_dict['CV_EVENTS']['Total'] = {'N':len(times_df),\n",
    "                                       'Median':median,\n",
    "                                       'Lower':lower_q, 'Upper':upper_q}   \n",
    "    demo_dict['CV_EVENTS']['Total']['Arrhythmia'] = processed_events['icd10_code'].str.startswith(tuple(ARYTHMIA_CODES)).sum().sum()\n",
    "    demo_dict['CV_EVENTS']['Total']['Valvular'] = processed_events['icd10_code'].str.startswith(tuple(VALV_CODES)).sum().sum()\n",
    "    demo_dict['CV_EVENTS']['Total']['ACS'] = processed_events['icd10_code'].str.startswith(tuple(ACS_CODES)).sum().sum()\n",
    "    demo_dict['CV_EVENTS']['Total']['CAD'] = processed_events['icd10_code'].str.startswith(tuple(CAD_CODES)).sum().sum()\n",
    "    demo_dict['CV_EVENTS']['Total']['Cerebrovascular'] = processed_events['icd10_code'].str.startswith(tuple(CEREBRO_CODES)).sum().sum()\n",
    "    demo_dict['CV_EVENTS']['Total']['Heart failure'] = processed_events['icd10_code'].str.startswith(tuple(HF_CODES)).sum().sum()\n",
    "    \n",
    "    print(demo_dict)\n",
    "    \n",
    "    def convert_to_native(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_native(v) for v in obj]\n",
    "        elif isinstance(obj, (np.integer, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64)):\n",
    "            return float(obj)\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        return obj\n",
    "\n",
    "    demo_dict_clean = convert_to_native(demo_dict)\n",
    "    \n",
    "    # save the cohort info to a json file\n",
    "    with open(os.path.join(outpath, 'cohort_info.json'), 'w') as f:\n",
    "        json.dump(demo_dict_clean, f, indent=4)   \n",
    "        \n",
    "# run\n",
    "get_cohort_info(cohort, processed_events, outpath)\n",
    "# save the wide time series cohort table\n",
    "cohort.to_csv(os.path.join(outpath, 'preprocessed_cohort_WIDE.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fcb7670-6b5e-456c-a306-1aab19906ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat the processed data table into format for model predictions\n",
    "# selecting a single time point from which to make a prediction for each patient\n",
    "\n",
    "# funcs\n",
    "def get_prediction_cohort(cohort):\n",
    "    '''\n",
    "        For each patient, take the follow up year with the least missingness.\n",
    "        If there is a tie, take the earlier one.\n",
    "        Set up the censoring dates\n",
    "    '''\n",
    "    pred_cohort = cohort[['person_id', 'transplant_date', 'CENSOR_DATE', 'sex', 'age_at_tx', 'SMOKER', 'METAB', 'ALD', \\\n",
    "                          'CANCER', 'HEP', 'FULM', 'IMMUNE', 'RE_TX']].copy()\n",
    "    \n",
    "    max_year = int((pd.to_datetime(STUDY_CUTOFF_DATE) - cohort['transplant_date'].min()).days / 365.25)\n",
    "    years = list(range(1, max_year + 1))\n",
    "    \n",
    "    years_to_censor = (cohort['CENSOR_DATE'] - cohort['transplant_date']).dt.days / 365.25\n",
    "    \n",
    "    missingness = pd.DataFrame(0,index=cohort.index, columns=years)\n",
    "    lab_cols = ['ALT', 'ALP', 'AST', 'BMI', 'CREATININE', 'CYCLO', 'TAC']\n",
    "    for col in lab_cols:\n",
    "        for year in years:\n",
    "            col_name = f'{col}_{year}'\n",
    "            # fill any value past years_to_censor with nan\n",
    "            cohort.loc[years_to_censor<year+0.25 ,col_name] = np.nan\n",
    "            missingness[year] += (cohort[col_name].isnull()).astype(int)\n",
    "    best_year = missingness.idxmin(axis=1)\n",
    "    \n",
    "    selected_values = []\n",
    "\n",
    "    varying_cols = lab_cols + ['DM', 'HTN', 'LIP', 'CV_HISTORY', 'ANTI_HTN', 'ANTI_PLATELET', 'STATIN', 'MONTHS_TO_EVENT']\n",
    "    for idx, year in best_year.items():\n",
    "        patient_values = []\n",
    "        for c in varying_cols:\n",
    "            col_name = f'{c}_{year}'\n",
    "            patient_values.append(cohort.at[idx, col_name])\n",
    "        selected_values.append(patient_values)\n",
    "        \n",
    "    pred_cohort[varying_cols] = pd.DataFrame(selected_values, columns=varying_cols)\n",
    "    pred_cohort['YRS_SINCE_TRANS'] = best_year + 0.25\n",
    "    pred_cohort['CURR_AGE'] = pred_cohort['age_at_tx']+pred_cohort['YRS_SINCE_TRANS']\n",
    "    \n",
    "    pred_cohort['EVENT'] = pred_cohort['MONTHS_TO_EVENT'].notnull().astype(int)\n",
    "    # fill null MONTHS_TO_EVENT with end point - anchor date (date of transplant + years since transplant)\n",
    "    # pred_cohort['anchor_dates'] = pred_cohort.apply(lambda row: row['transplant_date'] + pd.DateOffset(years=row['YRS_SINCE_TRANS'],months=3), axis=1)\n",
    "    pred_cohort['anchor_dates'] = pred_cohort['transplant_date'] + pd.to_timedelta(\n",
    "                                        (pred_cohort['YRS_SINCE_TRANS'] * 365.25).round().astype(int), unit=\"D\") \n",
    "    pred_cohort['MONTHS_TO_EVENT'] = (pred_cohort['MONTHS_TO_EVENT'].fillna((pred_cohort['CENSOR_DATE'] - pred_cohort['anchor_dates']).dt.days / 30.4)).round()\n",
    "    pred_cohort.drop(columns = ['transplant_date','CENSOR_DATE','anchor_dates'], inplace=True)\n",
    "    \n",
    "    pred_cohort.rename(columns={'age_at_tx':'AGE_AT_TX','person_id':'ID','sex':'SEX', 'CYCLO':'CYCLOSPORINE_TROUGH_LEVEL',\n",
    "                                'TAC':\"TACROLIMUS_TROUGH_LEVEL\",'CREATININE' : \"SERUM_CREATININE\"}, inplace=True)\n",
    "    \n",
    "    return pred_cohort\n",
    "\n",
    "# run\n",
    "# prediction_cohort = get_prediction_cohort(cohort)\n",
    "\n",
    "\n",
    "def get_prediction_cohort_random(cohort):\n",
    "    pred_cohort = cohort[['person_id', 'transplant_date', 'CENSOR_DATE', 'sex', 'age_at_tx', 'SMOKER', 'METAB', 'ALD', \\\n",
    "                          'CANCER', 'HEP', 'FULM', 'IMMUNE', 'RE_TX']].copy()\n",
    "    \n",
    "    max_year = int((pd.to_datetime(STUDY_CUTOFF_DATE) - cohort['transplant_date'].min()).days / 365.25)\n",
    "    years = list(range(1, max_year + 1))\n",
    "    \n",
    "    years_to_censor = (cohort['CENSOR_DATE'] - cohort['transplant_date']).dt.days / 365.25\n",
    "    \n",
    "    lab_cols = ['ALT', 'ALP', 'AST', 'BMI', 'CREATININE', 'CYCLO', 'TAC']\n",
    "    for col in lab_cols:\n",
    "        for year in years:\n",
    "            col_name = f'{col}_{year}'\n",
    "            # fill any value past years_to_censor with nan\n",
    "            cohort.loc[years_to_censor<(year+0.25) ,col_name] = np.nan\n",
    "    varying_cols = lab_cols + ['DM', 'HTN', 'LIP', 'CV_HISTORY', 'ANTI_HTN', 'ANTI_PLATELET', 'STATIN', 'MONTHS_TO_EVENT']\n",
    "    random_years=[]\n",
    "    for idx, censor_time in years_to_censor.items():\n",
    "        # eligible years = those that are before censor time\n",
    "        eligible_years = [y for y in years if censor_time >= (y+0.25)]\n",
    "        if len(eligible_years) == 0:\n",
    "            random_years.append(np.nan)\n",
    "        else:\n",
    "            random_years.append(np.random.choice(eligible_years))   \n",
    "    random_years = pd.Series(random_years, index=cohort.index, name=\"random_year\")\n",
    "    orig_len = pred_cohort.shape[0]\n",
    "    pred_cohort = pred_cohort[~random_years.isna()]\n",
    "    filtered_len = pred_cohort.shape[0] \n",
    "    random_years = random_years.dropna()\n",
    "    print(f'Dropped {orig_len - filtered_len} patients with no eligible follow-up year for random selection.')\n",
    "    selected_values = []\n",
    "    for idx, year in random_years.items():\n",
    "        patient_values = []\n",
    "        if pd.isna(year):\n",
    "            patient_values = [np.nan] * len(varying_cols)\n",
    "        else:\n",
    "            for c in varying_cols:\n",
    "                col_name = f'{c}_{int(year)}'\n",
    "                val = cohort.at[idx, col_name] if col_name in cohort.columns else np.nan\n",
    "                patient_values.append(val)\n",
    "        selected_values.append(patient_values)\n",
    "\n",
    "    pred_cohort[varying_cols] = pd.DataFrame(selected_values, columns=varying_cols)\n",
    "    pred_cohort['YRS_SINCE_TRANS'] = random_years + 0.25\n",
    "    pred_cohort['CURR_AGE'] = pred_cohort['age_at_tx']+pred_cohort['YRS_SINCE_TRANS']\n",
    "    \n",
    "    pred_cohort['EVENT'] = pred_cohort['MONTHS_TO_EVENT'].notnull().astype(int)\n",
    "    # fill null MONTHS_TO_EVENT with end point - anchor date (date of transplant + years since transplant)\n",
    "    pred_cohort['anchor_dates'] = pred_cohort['transplant_date'] + pd.to_timedelta(\n",
    "                                        (pred_cohort['YRS_SINCE_TRANS'] * 365.25).round().astype(int), unit=\"D\") #pred_cohort.apply(lambda row: row['transplant_date'] + pd.DateOffset(days = int(row['YRS_SINCE_TRANS']*365.25)), axis=1)\n",
    "    pred_cohort['MONTHS_TO_EVENT'] = (pred_cohort['MONTHS_TO_EVENT'].fillna((pred_cohort['CENSOR_DATE'] - pred_cohort['anchor_dates']).dt.days / 30.4)).round()\n",
    "    pred_cohort.drop(columns = ['transplant_date','CENSOR_DATE','anchor_dates'], inplace=True)\n",
    "    \n",
    "    pred_cohort.rename(columns={'age_at_tx':'AGE_AT_TX','person_id':'ID','sex':'SEX', 'CYCLO':'CYCLOSPORINE_TROUGH_LEVEL',\n",
    "                                'TAC':\"TACROLIMUS_TROUGH_LEVEL\",'CREATININE' : \"SERUM_CREATININE\"}, inplace=True)\n",
    "    \n",
    "    return pred_cohort\n",
    "\n",
    "prediction_cohort = get_prediction_cohort_random(cohort)\n",
    "\n",
    "\n",
    "\n",
    "prediction_cohort.to_csv(os.path.join(outpath, 'prediction_cohort.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1f1c7-c228-49bb-8b42-4d2fbad585e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transplant)",
   "language": "python",
   "name": "transplant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
